{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c801e6-512f-40e6-899f-3acafa5c22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "#normalizzazione ecc\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "DATASET_DIR = \"C:\\\\Users\\\\andre\\\\Desktop\\\\CNNModel\\\\dataset\"\n",
    "\n",
    "def get_dataset(dataset_dir=DATASET_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED):\n",
    "    \"\"\"\n",
    "    Loads image dataset from directory and splits it into training, validation, and test sets.\n",
    "    Applies normalization to all datasets and augmentation to the training set.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_dir}\")\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n",
    "\n",
    "    # Load the full dataset\n",
    "    # shuffle=False initially to ensure consistent splits based on take/skip\n",
    "    full_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        seed=seed,\n",
    "        image_size=img_size,\n",
    "        interpolation='nearest', # Use 'nearest' or 'bilinear'\n",
    "        shuffle=True, # Shuffle the initial dataset for better distribution\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    class_names = full_ds.class_names\n",
    "    num_batches = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "    print(f\"Found {len(full_ds.file_paths)} images belonging to {len(class_names)} classes.\")\n",
    "    print(f\"Class names: {class_names}\")\n",
    "    print(f\"Total batches in full dataset: {num_batches}\")\n",
    "\n",
    "    # Calculate split sizes\n",
    "    train_size = int(0.6 * num_batches)\n",
    "    val_size = int(0.2 * num_batches)\n",
    "    test_size = num_batches - train_size - val_size # Use remaining for test\n",
    "\n",
    "    print(f\"Train batches: {train_size}, Validation batches: {val_size}, Test batches: {test_size}\")\n",
    "\n",
    "    # Perform the split\n",
    "    train_ds = full_ds.take(train_size)\n",
    "    val_ds = full_ds.skip(train_size).take(val_size)\n",
    "    test_ds = full_ds.skip(train_size + val_size)\n",
    "\n",
    "    # Data Augmentation Layer (applied only to training data)\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomBrightness(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "        # tf.keras.layers.RandomRotation(0.1), # Optional, uncomment if needed\n",
    "        # tf.keras.layers.RandomZoom(0.1),    # Optional, uncomment if needed\n",
    "    ], name='data_augmentation_layer')\n",
    "\n",
    "    # Normalization Layer\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "    # Apply preprocessing pipeline\n",
    "    def preprocess_train(image, label):\n",
    "        image = data_augmentation(image) # Apply augmentation\n",
    "        image = normalization_layer(image) # Apply normalization\n",
    "        return image, label\n",
    "\n",
    "    def preprocess_val_test(image, label):\n",
    "        image = normalization_layer(image) # Only apply normalization\n",
    "        return image, label\n",
    "\n",
    "    # Map preprocessing functions and optimize performance\n",
    "    train_ds = train_ds.map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(preprocess_val_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.map(preprocess_val_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Prefetch for performance\n",
    "    train_ds = train_ds.cache().shuffle(buffer_size=1000, seed=seed).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Dataset preprocessing complete.\")\n",
    "    return train_ds, val_ds, test_ds, class_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage\n",
    "    train_ds, val_ds, test_ds, class_names = get_dataset()\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    print(f\"First batch from train_ds: {next(iter(train_ds))[0].shape}\")\n",
    "    print(f\"First batch from val_ds: {next(iter(val_ds))[0].shape}\")\n",
    "    print(f\"First batch from test_ds: {next(iter(test_ds))[0].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu_win)",
   "language": "python",
   "name": "tf_gpu_win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
